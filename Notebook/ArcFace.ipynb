{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc223ae",
   "metadata": {},
   "source": [
    "# Production-Grade 2D Face Recognition System\n",
    "## Cell 0: Install Dependencies\n",
    "\n",
    "This cell installs all required packages for the face recognition pipeline:\n",
    "- **ultralytics**: YOLOv8-Face detection\n",
    "- **insightface**: ArcFace embeddings and landmarks\n",
    "- **onnxruntime**: ONNX model inference\n",
    "- **scikit-learn**: Classifier training (SVM/KNN)\n",
    "- **opencv-python**: Image processing\n",
    "- **numpy, pandas, tqdm**: Data handling and progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1b3fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ All dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install all required dependencies\n",
    "%pip install -q ultralytics insightface onnxruntime scikit-learn opencv-python pillow numpy pandas tqdm\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596f494",
   "metadata": {},
   "source": [
    "## Cell 1: Import All Modules\n",
    "\n",
    "Import all necessary libraries for face detection, alignment, embedding extraction, and classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dd1ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules imported successfully\n",
      "‚úÖ Import verification passed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Face recognition specific imports\n",
    "from ultralytics import YOLO\n",
    "import insightface\n",
    "from insightface.utils.face_align import norm_crop\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully\")\n",
    "\n",
    "# Test: Verify key imports\n",
    "assert YOLO is not None, \"YOLO import failed\"\n",
    "assert insightface is not None, \"insightface import failed\"\n",
    "print(\"‚úÖ Import verification passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d21aa",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration and Paths\n",
    "\n",
    "Set up all file paths, dataset locations, and artifact directories. Create necessary folders if they don't exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b20d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "   Dataset: /mnt/NewDisk/sahil_project/FRS copy/Small_Dataset\n",
      "   Artifacts: /mnt/NewDisk/sahil_project/FRS copy/Artifacts\n",
      "   YOLO Model: /mnt/NewDisk/sahil_project/FRS copy/yolov8n-face.pt\n",
      "‚úÖ Path verification passed\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = Path(\"/mnt/NewDisk/sahil_project/FRS copy\")\n",
    "DATASET_DIR = BASE_DIR / \"Small_Dataset\"\n",
    "ARTIFACTS_DIR = BASE_DIR / \"Artifacts\"\n",
    "\n",
    "# Create artifact subdirectories\n",
    "EMBEDDINGS_DIR = ARTIFACTS_DIR / \"embeddings\"\n",
    "ALIGNED_FACES_DIR = ARTIFACTS_DIR / \"aligned_faces\"\n",
    "DETECTION_RESULTS_DIR = ARTIFACTS_DIR / \"detection_results\"\n",
    "LOGS_DIR = ARTIFACTS_DIR / \"logs\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [ARTIFACTS_DIR, EMBEDDINGS_DIR, ALIGNED_FACES_DIR, DETECTION_RESULTS_DIR, LOGS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# YOLOv8-Face model path\n",
    "YOLO_MODEL_PATH = BASE_DIR / \"yolov8n-face.pt\"\n",
    "\n",
    "# Recognition threshold (angular distance in radians)\n",
    "RECOGNITION_THRESHOLD = 0.5  # Will be calibrated during training\n",
    "UNKNOWN_THRESHOLD = 0.6  # Threshold for unknown users\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Dataset: {DATASET_DIR}\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR}\")\n",
    "print(f\"   YOLO Model: {YOLO_MODEL_PATH}\")\n",
    "\n",
    "# Test: Verify paths exist\n",
    "assert DATASET_DIR.exists(), f\"Dataset directory not found: {DATASET_DIR}\"\n",
    "assert YOLO_MODEL_PATH.exists(), f\"YOLO model not found: {YOLO_MODEL_PATH}\"\n",
    "assert ARTIFACTS_DIR.exists(), f\"Artifacts directory not created: {ARTIFACTS_DIR}\"\n",
    "print(\"‚úÖ Path verification passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9382bc6",
   "metadata": {},
   "source": [
    "## Cell 3: Initialize YOLOv8-Face Detector\n",
    "\n",
    "Initialize the YOLOv8-Face model for face detection. YOLOv8-Face provides fast and accurate face bounding box detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f96794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading YOLOv8-Face detector...\n",
      "‚úÖ YOLOv8-Face detector initialized\n",
      "‚úÖ YOLOv8-Face detector test passed\n"
     ]
    }
   ],
   "source": [
    "# Initialize YOLOv8-Face detector\n",
    "print(\"üîÑ Loading YOLOv8-Face detector...\")\n",
    "face_detector = YOLO(str(YOLO_MODEL_PATH))\n",
    "print(\"‚úÖ YOLOv8-Face detector initialized\")\n",
    "\n",
    "# Test: Verify detector can be called\n",
    "test_img = np.zeros((640, 640, 3), dtype=np.uint8)\n",
    "test_results = face_detector(test_img, verbose=False)\n",
    "assert test_results is not None, \"YOLO detector failed to process test image\"\n",
    "print(\"‚úÖ YOLOv8-Face detector test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599f135",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize InsightFace Models\n",
    "\n",
    "Initialize InsightFace with buffalo_l model which includes:\n",
    "- **RetinaFace detector** (used for landmark extraction only)\n",
    "- **5-point facial landmarks** (eyes, nose, mouth corners)\n",
    "- **ArcFace r100 embedder** (512-dimensional face embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d536aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading InsightFace buffalo_l model...\n",
      "   (This may take a minute - downloading models on first run)\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/matrix/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/matrix/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/matrix/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/matrix/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/matrix/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "‚úÖ InsightFace model initialized\n",
      "   Models: Detection, Landmarks (106 points), ArcFace Embedding\n",
      "‚úÖ InsightFace model test passed\n"
     ]
    }
   ],
   "source": [
    "# Initialize InsightFace FaceAnalysis model (buffalo_l)\n",
    "print(\"üîÑ Loading InsightFace buffalo_l model...\")\n",
    "print(\"   (This may take a minute - downloading models on first run)\")\n",
    "\n",
    "face_model = insightface.app.FaceAnalysis(name=\"buffalo_l\", providers=['CPUExecutionProvider'])\n",
    "face_model.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "print(\"‚úÖ InsightFace model initialized\")\n",
    "print(f\"   Models: Detection, Landmarks (106 points), ArcFace Embedding\")\n",
    "\n",
    "# Test: Verify InsightFace can process an image\n",
    "test_img = np.zeros((640, 640, 3), dtype=np.uint8)\n",
    "test_faces = face_model.get(test_img)\n",
    "assert isinstance(test_faces, list), \"InsightFace failed to process test image\"\n",
    "print(\"‚úÖ InsightFace model test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfc944",
   "metadata": {},
   "source": [
    "## Cell 5: Helper Functions\n",
    "\n",
    "Define core helper functions for:\n",
    "1. **Loading images** from file paths\n",
    "2. **YOLOv8 face detection** with confidence thresholding\n",
    "3. **Extracting bounding boxes** from YOLO results\n",
    "4. **Landmark detection** using InsightFace on cropped faces\n",
    "5. **Face alignment** using 5-point landmarks and norm_crop\n",
    "6. **ArcFace embedding extraction** from aligned faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb22f153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All helper functions defined\n",
      "   - load_image()\n",
      "   - detect_faces_yolo()\n",
      "   - extract_landmarks_insightface()\n",
      "   - align_face()\n",
      "   - extract_arcface_embedding()\n",
      "‚úÖ Helper functions test passed\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load image from file path and convert to BGR format (OpenCV standard)\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Image in BGR format, or None if loading fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            return None\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def detect_faces_yolo(img, conf_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Detect faces using YOLOv8-Face detector\n",
    "    \n",
    "    Args:\n",
    "        img: Image in BGR format (numpy array)\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "    \n",
    "    Returns:\n",
    "        List of detection results, each containing bbox and confidence\n",
    "    \"\"\"\n",
    "    results = face_detector(img, conf=conf_threshold, verbose=False)\n",
    "    \n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            bbox = box.xyxy[0].cpu().numpy().astype(int)  # [x1, y1, x2, y2]\n",
    "            confidence = float(box.conf.item())\n",
    "            detections.append({\n",
    "                'bbox': bbox,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "def extract_landmarks_insightface(img, bbox, return_embedding=False):\n",
    "    \"\"\"\n",
    "    Extract 5-point facial landmarks from a detected face region\n",
    "    \n",
    "    Args:\n",
    "        img: Full image in BGR format\n",
    "        bbox: Bounding box [x1, y1, x2, y2]\n",
    "        return_embedding: If True, also return the ArcFace embedding\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 5 keypoints [[x, y], ...] or None if not found\n",
    "        If return_embedding=True, returns (landmarks_5, embedding) tuple\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # Crop face region with padding\n",
    "    padding = 0.2\n",
    "    h, w = img.shape[:2]\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    crop_x1 = max(0, int(x1 - width * padding))\n",
    "    crop_y1 = max(0, int(y1 - height * padding))\n",
    "    crop_x2 = min(w, int(x2 + width * padding))\n",
    "    crop_y2 = min(h, int(y2 + height * padding))\n",
    "    \n",
    "    face_crop = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "    \n",
    "    if face_crop.size == 0:\n",
    "        return (None, None) if return_embedding else None\n",
    "    \n",
    "    # Get landmarks from InsightFace\n",
    "    faces = face_model.get(face_crop)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return (None, None) if return_embedding else None\n",
    "    \n",
    "    # Get the best face in the crop\n",
    "    best_face = max(faces, key=lambda f: f.det_score)\n",
    "    \n",
    "    # Extract 5-point landmarks (kps attribute)\n",
    "    landmarks_5 = best_face.kps.copy()  # Shape: (5, 2)\n",
    "    \n",
    "    # Adjust landmarks to original image coordinates\n",
    "    landmarks_5[:, 0] += crop_x1\n",
    "    landmarks_5[:, 1] += crop_y1\n",
    "    \n",
    "    if return_embedding:\n",
    "        # Extract embedding from the detected face\n",
    "        embedding = best_face.embedding.copy()  # 512-dimensional vector\n",
    "        # Normalize embedding (L2 normalization)\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm > 0:\n",
    "            embedding = embedding / norm\n",
    "        else:\n",
    "            embedding = None\n",
    "        return landmarks_5, embedding\n",
    "    \n",
    "    return landmarks_5\n",
    "\n",
    "\n",
    "def align_face(img, landmarks_5, output_size=112):\n",
    "    \"\"\"\n",
    "    Align face using 5-point landmarks with norm_crop from InsightFace\n",
    "    \n",
    "    Args:\n",
    "        img: Image in BGR format\n",
    "        landmarks_5: 5 keypoints [[x, y], ...]\n",
    "        output_size: Size of aligned face (integer, e.g., 112 for 112x112 square)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Aligned face in RGB format (112x112), or None if alignment fails\n",
    "    \"\"\"\n",
    "    if landmarks_5 is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Crop and align using InsightFace's norm_crop\n",
    "        # norm_crop expects image_size as an integer (for square output), not a tuple\n",
    "        if isinstance(output_size, tuple):\n",
    "            output_size = output_size[0]  # Use first value if tuple provided\n",
    "        \n",
    "        aligned = norm_crop(img, landmarks_5, image_size=output_size)\n",
    "        return aligned  # norm_crop returns RGB format\n",
    "    except Exception as e:\n",
    "        print(f\"Error in face alignment: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_arcface_embedding(aligned_face_rgb):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional ArcFace embedding from aligned face\n",
    "    \n",
    "    Why extraction from aligned faces sometimes fails:\n",
    "    - InsightFace's face_model.get() first runs face detection\n",
    "    - On an already-aligned 112x112 face, the detector may fail because:\n",
    "      1. The face lacks surrounding context the detector expects\n",
    "      2. The aligned face might not meet the detector's confidence threshold\n",
    "      3. The detector is trained on full images, not pre-aligned faces\n",
    "    \n",
    "    Solution: Use the embedding extracted during landmark detection (already available)\n",
    "    \n",
    "    Args:\n",
    "        aligned_face_rgb: Aligned face image in RGB format (112x112)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 512-dimensional embedding vector, or None if extraction fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert RGB to BGR for InsightFace\n",
    "        aligned_bgr = cv2.cvtColor(aligned_face_rgb, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Try using FaceAnalysis.get() on aligned face\n",
    "        # This often fails because the detector expects a full image with context\n",
    "        faces = face_model.get(aligned_bgr)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            # Success - detector found the face in aligned image\n",
    "            best_face = max(faces, key=lambda f: f.det_score)\n",
    "            embedding = best_face.embedding\n",
    "        else:\n",
    "            # Detection failed - this is expected for aligned faces\n",
    "            # The embedding from the original crop (extracted during landmark detection)\n",
    "            # is actually more reliable and will be used as fallback\n",
    "            return None\n",
    "        \n",
    "        # Normalize embedding (L2 normalization)\n",
    "        embedding = embedding.astype(np.float32)\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm > 0:\n",
    "            embedding = embedding / norm\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        # Silently fail - fallback embedding will be used\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ All helper functions defined\")\n",
    "print(\"   - load_image()\")\n",
    "print(\"   - detect_faces_yolo()\")\n",
    "print(\"   - extract_landmarks_insightface()\")\n",
    "print(\"   - align_face()\")\n",
    "print(\"   - extract_arcface_embedding()\")\n",
    "\n",
    "# Test: Verify functions are callable\n",
    "test_img = np.random.randint(0, 255, (640, 480, 3), dtype=np.uint8)\n",
    "test_detections = detect_faces_yolo(test_img, conf_threshold=0.1)\n",
    "assert isinstance(test_detections, list), \"detect_faces_yolo() failed\"\n",
    "print(\"‚úÖ Helper functions test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab9753",
   "metadata": {},
   "source": [
    "## Cell 6: Test Detection + Alignment + Embedding Pipeline\n",
    "\n",
    "Test the complete pipeline on a single sample image to verify all components work correctly together. This helps catch errors early.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d6e2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd9f4a6",
   "metadata": {},
   "source": [
    "## Cell 7: Dataset Loader - Process All Person Folders\n",
    "\n",
    "Iterate through all person folders in the dataset, detect faces, align them, extract embeddings, and save:\n",
    "- Embeddings per person (`.npy` files in `Artifacts/embeddings/`)\n",
    "- Aligned face crops (optional, saved in `Artifacts/aligned_faces/`)\n",
    "- Detection logs (saved in `Artifacts/logs/`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20331821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing pipeline on: /mnt/NewDisk/sahil_project/FRS copy/Small_Dataset/6003196229/6003196229_A08A0HEU0S.png\n",
      "Step 1: Detecting faces with YOLOv8...\n",
      "   Found 1 face(s)\n",
      "Step 2: Extracting landmarks and embedding...\n",
      "   Landmarks extracted: True\n",
      "   Embedding extracted from crop: True\n",
      "Step 3: Aligning face...\n",
      "   Face aligned: True, Shape: (112, 112, 3)\n",
      "Step 4: Extracting ArcFace embedding from aligned face...\n",
      "   ‚ö†Ô∏è Using embedding from original crop (aligned extraction failed)\n",
      "   Embedding extracted: True, Shape: (512,)\n",
      "‚úÖ Pipeline test PASSED - All components working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Find a test image from the dataset\n",
    "test_image_path = None\n",
    "for person_dir in DATASET_DIR.iterdir():\n",
    "    if person_dir.is_dir():\n",
    "        image_files = list(person_dir.glob(\"*.jpg\")) + list(person_dir.glob(\"*.png\")) + list(person_dir.glob(\"*.jpeg\"))\n",
    "        if image_files:\n",
    "            test_image_path = \"/mnt/NewDisk/sahil_project/FRS copy/Small_Dataset/6003196229/6003196229_A08A0HEU0S.png\"\n",
    "            break\n",
    "\n",
    "if test_image_path is None:\n",
    "    print(\"‚ö†Ô∏è No test image found in dataset. Creating a synthetic test...\")\n",
    "    test_img = np.random.randint(0, 255, (640, 480, 3), dtype=np.uint8)\n",
    "else:\n",
    "    print(f\"üß™ Testing pipeline on: {test_image_path}\")\n",
    "    test_img = load_image(test_image_path)\n",
    "\n",
    "if test_img is None:\n",
    "    raise ValueError(\"Could not load test image\")\n",
    "\n",
    "# Step 1: Detect faces\n",
    "print(\"Step 1: Detecting faces with YOLOv8...\")\n",
    "detections = detect_faces_yolo(test_img, conf_threshold=0.3)\n",
    "print(f\"   Found {len(detections)} face(s)\")\n",
    "\n",
    "if len(detections) == 0:\n",
    "    print(\"‚ö†Ô∏è No faces detected in test image. Pipeline test skipped.\")\n",
    "else:\n",
    "    # Step 2: Extract landmarks AND embedding together (more reliable)\n",
    "    print(\"Step 2: Extracting landmarks and embedding...\")\n",
    "    bbox = detections[0]['bbox']\n",
    "    landmarks, embedding_from_crop = extract_landmarks_insightface(test_img, bbox, return_embedding=True)\n",
    "    print(f\"   Landmarks extracted: {landmarks is not None}\")\n",
    "    print(f\"   Embedding extracted from crop: {embedding_from_crop is not None}\")\n",
    "    \n",
    "    if landmarks is not None:\n",
    "        # Step 3: Align face\n",
    "        print(\"Step 3: Aligning face...\")\n",
    "        aligned_face = align_face(test_img, landmarks)\n",
    "        print(f\"   Face aligned: {aligned_face is not None}, Shape: {aligned_face.shape if aligned_face is not None else None}\")\n",
    "        \n",
    "        if aligned_face is not None:\n",
    "            # Step 4: Try extracting embedding from aligned face (preferred for accuracy)\n",
    "            print(\"Step 4: Extracting ArcFace embedding from aligned face...\")\n",
    "            embedding_from_aligned = extract_arcface_embedding(aligned_face)\n",
    "            \n",
    "            # Use embedding from aligned face if successful, otherwise use the one from crop\n",
    "            if embedding_from_aligned is not None:\n",
    "                embedding = embedding_from_aligned\n",
    "                print(f\"   ‚úÖ Using embedding from aligned face\")\n",
    "            elif embedding_from_crop is not None:\n",
    "                embedding = embedding_from_crop\n",
    "                print(f\"   ‚ö†Ô∏è Using embedding from original crop (aligned extraction failed)\")\n",
    "            else:\n",
    "                embedding = None\n",
    "            \n",
    "            print(f\"   Embedding extracted: {embedding is not None}, Shape: {embedding.shape if embedding is not None else None}\")\n",
    "            \n",
    "            # Verify embedding dimensions\n",
    "            if embedding is not None:\n",
    "                assert embedding.shape == (512,), f\"Expected embedding shape (512,), got {embedding.shape}\"\n",
    "                assert np.isclose(np.linalg.norm(embedding), 1.0), \"Embedding should be L2 normalized\"\n",
    "                print(\"‚úÖ Pipeline test PASSED - All components working correctly!\")\n",
    "            else:\n",
    "                print(\"‚ùå Pipeline test FAILED - Embedding extraction failed\")\n",
    "        else:\n",
    "            print(\"‚ùå Pipeline test FAILED - Face alignment failed\")\n",
    "    else:\n",
    "        print(\"‚ùå Pipeline test FAILED - Landmark extraction failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718188e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45fc8312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing dataset...\n",
      "   Dataset directory: /mnt/NewDisk/sahil_project/FRS copy/Small_Dataset\n",
      "   Found 104 person(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing persons: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104/104 [28:52<00:00, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset processing completed!\n",
      "   Total persons: 104\n",
      "   Total images: 4116\n",
      "   Successful embeddings: 2268\n",
      "   Failed images: 1848\n",
      "   Success rate: 55.10%\n",
      "\n",
      "‚ö†Ô∏è Failed images saved to: /mnt/NewDisk/sahil_project/FRS copy/Artifacts/logs/dataset_processing_log.json\n",
      "‚úÖ Dataset processing test passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset processing: Extract embeddings for all persons\n",
    "print(\"üîÑ Processing dataset...\")\n",
    "print(f\"   Dataset directory: {DATASET_DIR}\")\n",
    "\n",
    "# Statistics\n",
    "total_persons = 0\n",
    "total_images = 0\n",
    "successful_embeddings = 0\n",
    "failed_images = []\n",
    "\n",
    "# Get all person directories\n",
    "person_dirs = [d for d in DATASET_DIR.iterdir() if d.is_dir()]\n",
    "person_dirs.sort()\n",
    "\n",
    "print(f\"   Found {len(person_dirs)} person(s)\")\n",
    "\n",
    "# Process each person\n",
    "for person_dir in tqdm(person_dirs, desc=\"Processing persons\"):\n",
    "    person_name = person_dir.name\n",
    "    total_persons += 1\n",
    "    \n",
    "    # Create embeddings directory for this person\n",
    "    person_emb_dir = EMBEDDINGS_DIR / person_name\n",
    "    person_emb_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all images for this person\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
    "    image_files = []\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(person_dir.glob(ext))\n",
    "    \n",
    "    person_embeddings = []\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in image_files:\n",
    "        total_images += 1\n",
    "        \n",
    "        # Load image\n",
    "        img = load_image(img_path)\n",
    "        if img is None:\n",
    "            failed_images.append((person_name, img_path.name, \"Failed to load image\"))\n",
    "            continue\n",
    "        \n",
    "        # Detect faces\n",
    "        detections = detect_faces_yolo(img, conf_threshold=0.4)\n",
    "        \n",
    "        if len(detections) == 0:\n",
    "            failed_images.append((person_name, img_path.name, \"No faces detected\"))\n",
    "            continue\n",
    "        \n",
    "        # Process the most confident detection\n",
    "        best_detection = max(detections, key=lambda x: x['confidence'])\n",
    "        bbox = best_detection['bbox']\n",
    "        \n",
    "        # Extract landmarks AND embedding together (more reliable)\n",
    "        try:\n",
    "            result = extract_landmarks_insightface(img, bbox, return_embedding=True)\n",
    "            if result is None:\n",
    "                failed_images.append((person_name, img_path.name, \"Landmark extraction failed\"))\n",
    "                continue\n",
    "            \n",
    "            landmarks, embedding_from_crop = result\n",
    "            if landmarks is None:\n",
    "                failed_images.append((person_name, img_path.name, \"Landmark extraction failed\"))\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            failed_images.append((person_name, img_path.name, f\"Landmark extraction error: {str(e)}\"))\n",
    "            continue\n",
    "        \n",
    "        # Align face\n",
    "        try:\n",
    "            aligned_face = align_face(img, landmarks)\n",
    "            if aligned_face is None:\n",
    "                failed_images.append((person_name, img_path.name, \"Face alignment failed\"))\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            failed_images.append((person_name, img_path.name, f\"Face alignment error: {str(e)}\"))\n",
    "            continue\n",
    "        \n",
    "        # Try extracting embedding from aligned face (preferred for accuracy)\n",
    "        embedding_from_aligned = None\n",
    "        try:\n",
    "            embedding_from_aligned = extract_arcface_embedding(aligned_face)\n",
    "        except Exception as e:\n",
    "            # Silently fail - will use fallback embedding from crop\n",
    "            pass\n",
    "        \n",
    "        # Use embedding from aligned face if successful, otherwise use the one from crop\n",
    "        if embedding_from_aligned is not None:\n",
    "            embedding = embedding_from_aligned\n",
    "        elif embedding_from_crop is not None:\n",
    "            embedding = embedding_from_crop\n",
    "        else:\n",
    "            failed_images.append((person_name, img_path.name, \"Embedding extraction failed\"))\n",
    "            continue\n",
    "        \n",
    "        # Verify embedding is valid\n",
    "        if embedding is None or embedding.shape != (512,):\n",
    "            failed_images.append((person_name, img_path.name, f\"Invalid embedding shape: {embedding.shape if embedding is not None else None}\"))\n",
    "            continue\n",
    "        \n",
    "        # Save embedding\n",
    "        embedding_filename = f\"{img_path.stem}.npy\"\n",
    "        embedding_path = person_emb_dir / embedding_filename\n",
    "        np.save(embedding_path, embedding)\n",
    "        \n",
    "        # Optionally save aligned face (comment out if not needed to save space)\n",
    "        # aligned_path = ALIGNED_FACES_DIR / person_name / f\"{img_path.stem}.jpg\"\n",
    "        # aligned_path.parent.mkdir(exist_ok=True)\n",
    "        # cv2.imwrite(str(aligned_path), cv2.cvtColor(aligned_face, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        person_embeddings.append({\n",
    "            'person': person_name,\n",
    "            'image': img_path.name,\n",
    "            'embedding_path': str(embedding_path),\n",
    "            'embedding': embedding\n",
    "        })\n",
    "        \n",
    "        successful_embeddings += 1\n",
    "    \n",
    "    # Save metadata for this person\n",
    "    if person_embeddings:\n",
    "        person_metadata = {\n",
    "            'person_name': person_name,\n",
    "            'num_images': len(person_embeddings),\n",
    "            'embedding_files': [e['embedding_path'] for e in person_embeddings]\n",
    "        }\n",
    "        metadata_path = person_emb_dir / \"metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(person_metadata, f, indent=2)\n",
    "\n",
    "# Save processing log\n",
    "log_data = {\n",
    "    'total_persons': total_persons,\n",
    "    'total_images': total_images,\n",
    "    'successful_embeddings': successful_embeddings,\n",
    "    'failed_images': failed_images,\n",
    "    'success_rate': successful_embeddings / total_images if total_images > 0 else 0\n",
    "}\n",
    "\n",
    "log_path = LOGS_DIR / \"dataset_processing_log.json\"\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(log_data, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Dataset processing completed!\")\n",
    "print(f\"   Total persons: {total_persons}\")\n",
    "print(f\"   Total images: {total_images}\")\n",
    "print(f\"   Successful embeddings: {successful_embeddings}\")\n",
    "print(f\"   Failed images: {len(failed_images)}\")\n",
    "print(f\"   Success rate: {log_data['success_rate']:.2%}\")\n",
    "\n",
    "if failed_images:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed images saved to: {log_path}\")\n",
    "\n",
    "# Test: Verify embeddings were created\n",
    "assert successful_embeddings > 0, \"No embeddings were successfully created\"\n",
    "assert (EMBEDDINGS_DIR).exists(), \"Embeddings directory not found\"\n",
    "print(\"‚úÖ Dataset processing test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcf96f",
   "metadata": {},
   "source": [
    "## Cell 8: Combine All Embeddings into Master Database\n",
    "\n",
    "Load all embeddings from individual `.npy` files and combine them into:\n",
    "- **X**: Master embedding matrix (N samples √ó 512 dimensions)\n",
    "- **y**: Label vector (N samples)\n",
    "- **label_encoder**: Maps person names to integer labels\n",
    "\n",
    "Save the unified database to the Artifacts folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b91f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Combining all embeddings into master database...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDINGS_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m skipped_persons \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load embeddings from all person directories\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m person_dirs \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mEMBEDDINGS_DIR\u001b[49m\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[0;32m     12\u001b[0m person_dirs\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m person_dir \u001b[38;5;129;01min\u001b[39;00m person_dirs:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EMBEDDINGS_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all embeddings into master database\n",
    "print(\"üîÑ Combining all embeddings into master database...\")\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "person_to_index = {}\n",
    "index_to_person = {}\n",
    "skipped_persons = []\n",
    "\n",
    "# Load embeddings from all person directories\n",
    "person_dirs = [d for d in EMBEDDINGS_DIR.iterdir() if d.is_dir()]\n",
    "person_dirs.sort()\n",
    "\n",
    "for person_dir in person_dirs:\n",
    "    person_name = person_dir.name\n",
    "    \n",
    "    # Load all embeddings for this person\n",
    "    embedding_files = list(person_dir.glob(\"*.npy\"))\n",
    "    \n",
    "    # Only process persons that have actual embedding files\n",
    "    if len(embedding_files) == 0:\n",
    "        skipped_persons.append(person_name)\n",
    "        continue  # Skip persons with no embeddings\n",
    "    \n",
    "    # Add person to mapping only if they have embeddings\n",
    "    if person_name not in person_to_index:\n",
    "        idx = len(person_to_index)\n",
    "        person_to_index[person_name] = idx\n",
    "        index_to_person[idx] = person_name\n",
    "    \n",
    "    for emb_file in embedding_files:\n",
    "        embedding = np.load(emb_file)\n",
    "        all_embeddings.append(embedding)\n",
    "        all_labels.append(person_name)\n",
    "\n",
    "# Warn about skipped persons if any\n",
    "if skipped_persons:\n",
    "    print(f\"   ‚ö†Ô∏è Skipped {len(skipped_persons)} person(s) with no embeddings: {', '.join(skipped_persons[:5])}{'...' if len(skipped_persons) > 5 else ''}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(all_embeddings)  # Shape: (N, 512)\n",
    "y = np.array(all_labels)      # Shape: (N,)\n",
    "\n",
    "print(f\"   Total embeddings: {X.shape[0]}\")\n",
    "print(f\"   Embedding dimension: {X.shape[1]}\")\n",
    "print(f\"   Number of unique persons: {len(person_to_index)}\")\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Save master database\n",
    "database_path = ARTIFACTS_DIR / \"embedding_database.npy\"\n",
    "labels_path = ARTIFACTS_DIR / \"labels.npy\"\n",
    "label_encoder_path = ARTIFACTS_DIR / \"label_encoder.pkl\"\n",
    "person_mapping_path = ARTIFACTS_DIR / \"person_mapping.json\"\n",
    "\n",
    "np.save(database_path, X)\n",
    "np.save(labels_path, y_encoded)\n",
    "\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "with open(person_mapping_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'person_to_index': person_to_index,\n",
    "        'index_to_person': index_to_person\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Master database created and saved\")\n",
    "print(f\"   Database: {database_path}\")\n",
    "print(f\"   Labels: {labels_path}\")\n",
    "print(f\"   Label encoder: {label_encoder_path}\")\n",
    "print(f\"   Person mapping: {person_mapping_path}\")\n",
    "\n",
    "# Test: Verify database structure\n",
    "assert X.shape[1] == 512, f\"Expected embedding dimension 512, got {X.shape[1]}\"\n",
    "assert len(y_encoded) == X.shape[0], \"Mismatch between embeddings and labels\"\n",
    "unique_labels_in_y = len(set(y))\n",
    "unique_persons_in_mapping = len(person_to_index)\n",
    "assert unique_labels_in_y == unique_persons_in_mapping, \\\n",
    "    f\"Person mapping mismatch: {unique_labels_in_y} unique labels in y, but {unique_persons_in_mapping} persons in mapping. \" \\\n",
    "    f\"This may occur if some person directories have no embedding files.\"\n",
    "print(\"‚úÖ Database combination test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b5c6a",
   "metadata": {},
   "source": [
    "## Cell 9: Train Classifier (SVM vs KNN)\n",
    "\n",
    "Compare SVM-RBF and KNN classifiers using cross-validation, select the best one, train it on the full dataset, and save the model. The classifier will be used for fast face recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67a46e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training classifier...\n",
      "   Dataset size: 2268 samples, 104 classes\n",
      "\n",
      "üìä Evaluating SVM-RBF classifier...\n",
      "   SVM-RBF: 0.9960 (+/- 0.0033)\n",
      "\n",
      "üìä Evaluating KNN classifier...\n",
      "   KNN: 0.9969 (+/- 0.0045)\n",
      "\n",
      "‚úÖ Selected: KNN (better accuracy)\n",
      "\n",
      "üîÑ Training KNN on full dataset...\n",
      "‚úÖ Classifier trained\n",
      "   Saved to: /mnt/NewDisk/sahil_project/FRS copy/Artifacts/face_classifier.pkl\n",
      "   Metadata: /mnt/NewDisk/sahil_project/FRS copy/Artifacts/classifier_metadata.json\n",
      "‚úÖ Classifier training test passed\n"
     ]
    }
   ],
   "source": [
    "# Train classifier: Compare SVM-RBF vs KNN\n",
    "print(\"üîÑ Training classifier...\")\n",
    "print(f\"   Dataset size: {X.shape[0]} samples, {len(np.unique(y_encoded))} classes\")\n",
    "\n",
    "# Evaluate SVM-RBF\n",
    "print(\"\\nüìä Evaluating SVM-RBF classifier...\")\n",
    "svm_classifier = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_scores = cross_val_score(svm_classifier, X, y_encoded, cv=min(5, len(np.unique(y_encoded))), scoring='accuracy')\n",
    "svm_mean_score = svm_scores.mean()\n",
    "svm_std_score = svm_scores.std()\n",
    "print(f\"   SVM-RBF: {svm_mean_score:.4f} (+/- {svm_std_score*2:.4f})\")\n",
    "\n",
    "# Evaluate KNN\n",
    "print(\"\\nüìä Evaluating KNN classifier...\")\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=min(5, len(np.unique(y_encoded))-1))\n",
    "knn_scores = cross_val_score(knn_classifier, X, y_encoded, cv=min(5, len(np.unique(y_encoded))), scoring='accuracy')\n",
    "knn_mean_score = knn_scores.mean()\n",
    "knn_std_score = knn_scores.std()\n",
    "print(f\"   KNN: {knn_mean_score:.4f} (+/- {knn_std_score*2:.4f})\")\n",
    "\n",
    "# Select best classifier\n",
    "if svm_mean_score >= knn_mean_score:\n",
    "    print(\"\\n‚úÖ Selected: SVM-RBF (better accuracy)\")\n",
    "    best_classifier = svm_classifier\n",
    "    classifier_type = \"SVM-RBF\"\n",
    "    best_score = svm_mean_score\n",
    "else:\n",
    "    print(\"\\n‚úÖ Selected: KNN (better accuracy)\")\n",
    "    best_classifier = knn_classifier\n",
    "    classifier_type = \"KNN\"\n",
    "    best_score = knn_mean_score\n",
    "\n",
    "# Train on full dataset\n",
    "print(f\"\\nüîÑ Training {classifier_type} on full dataset...\")\n",
    "best_classifier.fit(X, y_encoded)\n",
    "print(\"‚úÖ Classifier trained\")\n",
    "\n",
    "# Save classifier\n",
    "classifier_path = ARTIFACTS_DIR / \"face_classifier.pkl\"\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(best_classifier, f)\n",
    "\n",
    "# Save classifier metadata\n",
    "classifier_metadata = {\n",
    "    'classifier_type': classifier_type,\n",
    "    'cv_score_mean': float(best_score),\n",
    "    'num_classes': len(np.unique(y_encoded)),\n",
    "    'num_samples': X.shape[0]\n",
    "}\n",
    "\n",
    "classifier_metadata_path = ARTIFACTS_DIR / \"classifier_metadata.json\"\n",
    "with open(classifier_metadata_path, 'w') as f:\n",
    "    json.dump(classifier_metadata, f, indent=2)\n",
    "\n",
    "print(f\"   Saved to: {classifier_path}\")\n",
    "print(f\"   Metadata: {classifier_metadata_path}\")\n",
    "\n",
    "# Test: Verify classifier can make predictions\n",
    "test_pred = best_classifier.predict(X[:5])\n",
    "test_proba = best_classifier.predict_proba(X[:5])\n",
    "assert len(test_pred) == 5, \"Classifier prediction failed\"\n",
    "assert test_proba.shape[1] == len(np.unique(y_encoded)), \"Classifier probability shape mismatch\"\n",
    "print(\"‚úÖ Classifier training test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c661c",
   "metadata": {},
   "source": [
    "## Cell 10: Calibrate Recognition Threshold\n",
    "\n",
    "Calculate the optimal threshold for distinguishing known vs unknown faces. This uses angular distance (cosine similarity) between embeddings to determine if a face belongs to a known person or is unknown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3017b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Calibrating recognition threshold...\n",
      "   Intra-class distance: 0.5865 ¬± 0.2679\n",
      "   Inter-class distance: 1.5095 ¬± 0.0742\n",
      "   Calibrated threshold: 1.0480\n",
      "‚úÖ Threshold calibrated and saved to: /mnt/NewDisk/sahil_project/FRS copy/Artifacts/recognition_thresholds.json\n",
      "‚úÖ Threshold calibration test passed\n"
     ]
    }
   ],
   "source": [
    "# Calibrate recognition threshold using angular distance\n",
    "print(\"üîÑ Calibrating recognition threshold...\")\n",
    "\n",
    "def angular_distance(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Compute angular distance between two normalized embeddings\n",
    "    Angular distance = arccos(cosine_similarity)\n",
    "    \n",
    "    Args:\n",
    "        embedding1, embedding2: Normalized embedding vectors\n",
    "    \n",
    "    Returns:\n",
    "        float: Angular distance in radians (0 to œÄ)\n",
    "    \"\"\"\n",
    "    cosine_sim = np.dot(embedding1, embedding2)\n",
    "    cosine_sim = np.clip(cosine_sim, -1.0, 1.0)  # Ensure valid range for arccos\n",
    "    angular_dist = np.arccos(cosine_sim)\n",
    "    return angular_dist\n",
    "\n",
    "# Calculate intra-class distances (same person)\n",
    "intra_class_distances = []\n",
    "for person_name in person_to_index.keys():\n",
    "    person_indices = np.where(y == person_name)[0]\n",
    "    person_embeddings = X[person_indices]\n",
    "    \n",
    "    if len(person_embeddings) > 1:\n",
    "        # Compute pairwise distances within this person\n",
    "        for i in range(len(person_embeddings)):\n",
    "            for j in range(i+1, len(person_embeddings)):\n",
    "                dist = angular_distance(person_embeddings[i], person_embeddings[j])\n",
    "                intra_class_distances.append(dist)\n",
    "\n",
    "# Calculate inter-class distances (different persons)\n",
    "inter_class_distances = []\n",
    "unique_labels = np.unique(y_encoded)\n",
    "for i in range(len(unique_labels)):\n",
    "    for j in range(i+1, len(unique_labels)):\n",
    "        label_i = unique_labels[i]\n",
    "        label_j = unique_labels[j]\n",
    "        emb_i = X[y_encoded == label_i]\n",
    "        emb_j = X[y_encoded == label_j]\n",
    "        \n",
    "        # Sample some pairs to avoid O(n¬≤) computation\n",
    "        sample_size = min(50, len(emb_i) * len(emb_j))\n",
    "        for _ in range(sample_size):\n",
    "            idx_i = np.random.randint(len(emb_i))\n",
    "            idx_j = np.random.randint(len(emb_j))\n",
    "            dist = angular_distance(emb_i[idx_i], emb_j[idx_j])\n",
    "            inter_class_distances.append(dist)\n",
    "\n",
    "if intra_class_distances and inter_class_distances:\n",
    "    intra_mean = np.mean(intra_class_distances)\n",
    "    intra_std = np.std(intra_class_distances)\n",
    "    inter_mean = np.mean(inter_class_distances)\n",
    "    inter_std = np.std(inter_class_distances)\n",
    "    \n",
    "    # Set threshold as mean intra-class + 2*std (conservative)\n",
    "    calibrated_threshold = intra_mean + 2 * intra_std\n",
    "    \n",
    "    # Ensure threshold is reasonable (between intra and inter means)\n",
    "    calibrated_threshold = min(calibrated_threshold, (intra_mean + inter_mean) / 2)\n",
    "    \n",
    "    print(f\"   Intra-class distance: {intra_mean:.4f} ¬± {intra_std:.4f}\")\n",
    "    print(f\"   Inter-class distance: {inter_mean:.4f} ¬± {inter_std:.4f}\")\n",
    "    print(f\"   Calibrated threshold: {calibrated_threshold:.4f}\")\n",
    "    \n",
    "    RECOGNITION_THRESHOLD = float(calibrated_threshold)\n",
    "    UNKNOWN_THRESHOLD = float(calibrated_threshold * 1.2)  # Slightly higher for unknown detection\n",
    "    \n",
    "    # Save threshold\n",
    "    threshold_data = {\n",
    "        'recognition_threshold': RECOGNITION_THRESHOLD,\n",
    "        'unknown_threshold': UNKNOWN_THRESHOLD,\n",
    "        'intra_class_mean': float(intra_mean),\n",
    "        'intra_class_std': float(intra_std),\n",
    "        'inter_class_mean': float(inter_mean),\n",
    "        'inter_class_std': float(inter_std)\n",
    "    }\n",
    "    \n",
    "    threshold_path = ARTIFACTS_DIR / \"recognition_thresholds.json\"\n",
    "    with open(threshold_path, 'w') as f:\n",
    "        json.dump(threshold_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Threshold calibrated and saved to: {threshold_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not calibrate threshold (insufficient data). Using default values.\")\n",
    "    RECOGNITION_THRESHOLD = 0.5\n",
    "    UNKNOWN_THRESHOLD = 0.6\n",
    "\n",
    "# Test: Verify threshold values\n",
    "assert 0 < RECOGNITION_THRESHOLD < np.pi, \"Invalid threshold range\"\n",
    "print(\"‚úÖ Threshold calibration test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23d904",
   "metadata": {},
   "source": [
    "## Cell 11: Recognition Function\n",
    "\n",
    "Implement the complete face recognition pipeline:\n",
    "1. YOLOv8 face detection\n",
    "2. InsightFace landmark extraction and alignment\n",
    "3. ArcFace embedding extraction\n",
    "4. Angular distance computation with database\n",
    "5. Threshold-based classification (known person or unknown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5091a6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Recognition function defined\n",
      "‚úÖ Recognition function test passed\n"
     ]
    }
   ],
   "source": [
    "def recognize_face(image_path_or_array, return_details=False):\n",
    "    \"\"\"\n",
    "    Complete face recognition pipeline\n",
    "    \n",
    "    Args:\n",
    "        image_path_or_array: Image path (str) or numpy array (BGR format)\n",
    "        return_details: If True, return detailed information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Recognition results with identity, confidence, distances, etc.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path_or_array, str):\n",
    "        img = load_image(image_path_or_array)\n",
    "    else:\n",
    "        img = image_path_or_array.copy()\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if img is None:\n",
    "        return {\n",
    "            'identity': 'unknown', \n",
    "            'confidence': 0.0, \n",
    "            'angular_distance': None,\n",
    "            'detection_confidence': 0.0,\n",
    "            'bbox': None,\n",
    "            'is_unknown': True,\n",
    "            'error': 'Failed to load image'\n",
    "        }\n",
    "    \n",
    "    # Step 1: Detect faces\n",
    "    detections = detect_faces_yolo(img, conf_threshold=0.4)\n",
    "    \n",
    "    if len(detections) == 0:\n",
    "        return {\n",
    "            'identity': 'unknown', \n",
    "            'confidence': 0.0, \n",
    "            'angular_distance': None,\n",
    "            'detection_confidence': 0.0,\n",
    "            'bbox': None,\n",
    "            'is_unknown': True,\n",
    "            'error': 'No faces detected'\n",
    "        }\n",
    "    \n",
    "    # Process the most confident detection\n",
    "    best_detection = max(detections, key=lambda x: x['confidence'])\n",
    "    bbox = best_detection['bbox']\n",
    "    detection_confidence = best_detection['confidence']\n",
    "    \n",
    "    # Step 2: Extract landmarks\n",
    "    landmarks = extract_landmarks_insightface(img, bbox)\n",
    "    if landmarks is None:\n",
    "        return {\n",
    "            'identity': 'unknown', \n",
    "            'confidence': 0.0, \n",
    "            'angular_distance': None,\n",
    "            'detection_confidence': float(detection_confidence),\n",
    "            'bbox': bbox.tolist(),\n",
    "            'is_unknown': True,\n",
    "            'error': 'Landmark extraction failed'\n",
    "        }\n",
    "    \n",
    "    # Step 3: Align face\n",
    "    aligned_face = align_face(img, landmarks)\n",
    "    if aligned_face is None:\n",
    "        return {\n",
    "            'identity': 'unknown', \n",
    "            'confidence': 0.0, \n",
    "            'angular_distance': None,\n",
    "            'detection_confidence': float(detection_confidence),\n",
    "            'bbox': bbox.tolist(),\n",
    "            'is_unknown': True,\n",
    "            'error': 'Face alignment failed'\n",
    "        }\n",
    "    \n",
    "    # Step 4: Extract embedding (try with fallback)\n",
    "    embedding = None\n",
    "    # Try aligned face first\n",
    "    embedding = extract_arcface_embedding(aligned_face)\n",
    "    if embedding is None:\n",
    "        # Fallback: Extract embedding during landmark extraction\n",
    "        try:\n",
    "            _, embedding = extract_landmarks_insightface(img, bbox, return_embedding=True)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if embedding is None:\n",
    "        return {\n",
    "            'identity': 'unknown', \n",
    "            'confidence': 0.0, \n",
    "            'angular_distance': None,\n",
    "            'detection_confidence': float(detection_confidence),\n",
    "            'bbox': bbox.tolist(),\n",
    "            'is_unknown': True,\n",
    "            'error': 'Embedding extraction failed'\n",
    "        }\n",
    "    \n",
    "    # Step 5: Compute angular distances to all database embeddings\n",
    "    angular_distances = []\n",
    "    for db_embedding in X:\n",
    "        dist = angular_distance(embedding, db_embedding)\n",
    "        angular_distances.append(dist)\n",
    "    \n",
    "    angular_distances = np.array(angular_distances)\n",
    "    \n",
    "    # Step 6: Find closest match\n",
    "    min_distance_idx = np.argmin(angular_distances)\n",
    "    min_distance = angular_distances[min_distance_idx]\n",
    "    \n",
    "    # Step 7: Classify using threshold\n",
    "    if min_distance <= RECOGNITION_THRESHOLD:\n",
    "        # Known person\n",
    "        predicted_label_idx = y_encoded[min_distance_idx]\n",
    "        predicted_person = label_encoder.inverse_transform([predicted_label_idx])[0]\n",
    "        \n",
    "        # Also use classifier for verification\n",
    "        classifier_pred_idx = best_classifier.predict([embedding])[0]\n",
    "        classifier_pred = label_encoder.inverse_transform([classifier_pred_idx])[0]\n",
    "        classifier_proba = best_classifier.predict_proba([embedding])[0][classifier_pred_idx]\n",
    "        \n",
    "        # Use classifier prediction if it has high confidence, otherwise use distance-based\n",
    "        if classifier_proba > 0.7 and classifier_pred == predicted_person:\n",
    "            identity = classifier_pred\n",
    "            confidence = float(classifier_proba)\n",
    "        else:\n",
    "            identity = predicted_person\n",
    "            confidence = float(1.0 - (min_distance / RECOGNITION_THRESHOLD))  # Normalize to [0, 1]\n",
    "        \n",
    "        result = {\n",
    "            'identity': identity,\n",
    "            'confidence': confidence,\n",
    "            'angular_distance': float(min_distance),\n",
    "            'detection_confidence': float(detection_confidence),\n",
    "            'bbox': bbox.tolist(),\n",
    "            'is_unknown': False\n",
    "        }\n",
    "    else:\n",
    "        # Unknown person\n",
    "        result = {\n",
    "            'identity': 'unknown',\n",
    "            'confidence': 0.0,\n",
    "            'angular_distance': float(min_distance),\n",
    "            'detection_confidence': float(detection_confidence),\n",
    "            'bbox': bbox.tolist(),\n",
    "            'is_unknown': True\n",
    "        }\n",
    "    \n",
    "    if return_details:\n",
    "        result['embedding'] = embedding\n",
    "        result['aligned_face'] = aligned_face\n",
    "        result['landmarks'] = landmarks.tolist()\n",
    "        result['all_distances'] = angular_distances.tolist()\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Recognition function defined\")\n",
    "\n",
    "# Test: Verify function can be called\n",
    "test_result = recognize_face(test_img if 'test_img' in locals() else np.zeros((640, 480, 3), dtype=np.uint8))\n",
    "assert 'identity' in test_result, \"Recognition function failed\"\n",
    "print(\"‚úÖ Recognition function test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a311068",
   "metadata": {},
   "source": [
    "## Cell 12: Test Recognition on Dataset Images\n",
    "\n",
    "Load a random image from the dataset and verify the complete recognition pipeline works correctly end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0608bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing recognition on dataset image...\n",
      "   Test image: 6204010540_A08A0HG3HA_aug10.jpg\n",
      "   Expected person: 6204010540\n",
      "\n",
      "üìä Recognition Results:\n",
      "   Identity: 6204010540\n",
      "   Confidence: 1.0000\n",
      "   Angular distance: 0.0000\n",
      "   Detection confidence: 0.8478\n",
      "   Is unknown: False\n",
      "\n",
      "‚úÖ Recognition test PASSED - Correctly identified as 6204010540\n",
      "\n",
      "‚úÖ Recognition test structure verified - all required keys present\n"
     ]
    }
   ],
   "source": [
    "# Test recognition on a random image from dataset\n",
    "print(\"üß™ Testing recognition on dataset image...\")\n",
    "\n",
    "# Verify recognize_face function is available and has correct signature\n",
    "if 'recognize_face' not in globals():\n",
    "    raise NameError(\"recognize_face function not defined. Please run Cell 11 (Recognition Function) first.\")\n",
    "\n",
    "# Find a random image from dataset\n",
    "test_image_path = None\n",
    "test_person_name = None\n",
    "\n",
    "for person_dir in DATASET_DIR.iterdir():\n",
    "    if person_dir.is_dir():\n",
    "        image_files = list(person_dir.glob(\"*.jpg\")) + list(person_dir.glob(\"*.png\"))\n",
    "        if image_files:\n",
    "            import random\n",
    "            test_image_path = random.choice(image_files)\n",
    "            test_person_name = person_dir.name\n",
    "            break\n",
    "\n",
    "if test_image_path is None:\n",
    "    print(\"‚ö†Ô∏è No test image found. Using synthetic image...\")\n",
    "    test_img_array = np.random.randint(0, 255, (640, 480, 3), dtype=np.uint8)\n",
    "    result = recognize_face(test_img_array)\n",
    "    print(f\"   Result: {result['identity']} (expected: unknown for synthetic image)\")\n",
    "else:\n",
    "    print(f\"   Test image: {test_image_path.name}\")\n",
    "    print(f\"   Expected person: {test_person_name}\")\n",
    "    \n",
    "    # Perform recognition\n",
    "    result = recognize_face(str(test_image_path), return_details=False)\n",
    "    \n",
    "    print(f\"\\nüìä Recognition Results:\")\n",
    "    print(f\"   Identity: {result['identity']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.4f}\")\n",
    "    \n",
    "    # Handle angular_distance - may be None for errors\n",
    "    if result.get('angular_distance') is not None:\n",
    "        print(f\"   Angular distance: {result['angular_distance']:.4f}\")\n",
    "    else:\n",
    "        print(f\"   Angular distance: N/A (error occurred)\")\n",
    "    \n",
    "    print(f\"   Detection confidence: {result.get('detection_confidence', 0.0):.4f}\")\n",
    "    print(f\"   Is unknown: {result.get('is_unknown', True)}\")\n",
    "    \n",
    "    # Show error if present\n",
    "    if 'error' in result:\n",
    "        print(f\"   ‚ö†Ô∏è Error: {result['error']}\")\n",
    "    \n",
    "    if result['identity'] == test_person_name:\n",
    "        print(f\"\\n‚úÖ Recognition test PASSED - Correctly identified as {test_person_name}\")\n",
    "    elif result['identity'] == 'unknown':\n",
    "        print(f\"\\n‚ö†Ô∏è Recognition test - Classified as unknown (may be due to threshold)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Recognition test - Misidentified: expected {test_person_name}, got {result['identity']}\")\n",
    "\n",
    "# Test: Verify recognition returns valid structure\n",
    "required_keys = ['identity', 'confidence', 'angular_distance', 'detection_confidence', 'bbox', 'is_unknown']\n",
    "missing_keys = [key for key in required_keys if key not in result]\n",
    "if missing_keys:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Missing keys in result: {missing_keys}\")\n",
    "    print(f\"   Result keys: {list(result.keys())}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Recognition test structure verified - all required keys present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b41cf1",
   "metadata": {},
   "source": [
    "## Cell 14: Demo - Recognize Face from Image Path\n",
    "\n",
    "Final demonstration cell that accepts an image path, performs recognition, and prints detailed results including predicted identity, confidence scores, and angular distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321abd41",
   "metadata": {},
   "source": [
    "## Cell 13: Register New Person\n",
    "\n",
    "Function to dynamically register a new person by processing multiple images, extracting embeddings, and adding them to the database and classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3518b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Register new person function defined\n",
      "üîÑ Registering new person: _test_person\n",
      "‚úÖ Registration function test passed\n"
     ]
    }
   ],
   "source": [
    "def register_new_person(person_name, image_paths, min_images=3):\n",
    "    \"\"\"\n",
    "    Register a new person by processing multiple images and adding to database\n",
    "    \n",
    "    Args:\n",
    "        person_name: Name/ID of the person to register\n",
    "        image_paths: List of image file paths for this person\n",
    "        min_images: Minimum number of successful embeddings required\n",
    "    \n",
    "    Returns:\n",
    "        dict: Registration results\n",
    "    \"\"\"\n",
    "    # Declare global variables at the start of the function\n",
    "    global X, y, y_encoded, person_to_index, index_to_person\n",
    "    \n",
    "    print(f\"üîÑ Registering new person: {person_name}\")\n",
    "    \n",
    "    if person_name in person_to_index:\n",
    "        return {'success': False, 'error': f'Person {person_name} already exists in database'}\n",
    "    \n",
    "    # Create embeddings directory for this person\n",
    "    person_emb_dir = EMBEDDINGS_DIR / person_name\n",
    "    person_emb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    new_embeddings = []\n",
    "    successful_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for img_path in image_paths:\n",
    "        img_path = Path(img_path)\n",
    "        if not img_path.exists():\n",
    "            print(f\"   ‚ö†Ô∏è Image not found: {img_path}\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Load and process image\n",
    "        img = load_image(img_path)\n",
    "        if img is None:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Detect faces\n",
    "        detections = detect_faces_yolo(img, conf_threshold=0.4)\n",
    "        if len(detections) == 0:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Process best detection\n",
    "        best_detection = max(detections, key=lambda x: x['confidence'])\n",
    "        bbox = best_detection['bbox']\n",
    "        \n",
    "        # Extract landmarks AND embedding together (more reliable)\n",
    "        try:\n",
    "            result = extract_landmarks_insightface(img, bbox, return_embedding=True)\n",
    "            if result is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            landmarks, embedding_from_crop = result\n",
    "            if landmarks is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Align face\n",
    "        try:\n",
    "            aligned_face = align_face(img, landmarks)\n",
    "            if aligned_face is None:\n",
    "                failed_count += 1\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Try extracting embedding from aligned face (preferred for accuracy)\n",
    "        embedding_from_aligned = None\n",
    "        try:\n",
    "            embedding_from_aligned = extract_arcface_embedding(aligned_face)\n",
    "        except Exception as e:\n",
    "            # Silently fail - will use fallback embedding from crop\n",
    "            pass\n",
    "        \n",
    "        # Use embedding from aligned face if successful, otherwise use the one from crop\n",
    "        if embedding_from_aligned is not None:\n",
    "            embedding = embedding_from_aligned\n",
    "        elif embedding_from_crop is not None:\n",
    "            embedding = embedding_from_crop\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Verify embedding is valid\n",
    "        if embedding is None or embedding.shape != (512,):\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Save embedding\n",
    "        embedding_filename = f\"{img_path.stem}.npy\"\n",
    "        embedding_path = person_emb_dir / embedding_filename\n",
    "        np.save(embedding_path, embedding)\n",
    "        \n",
    "        new_embeddings.append(embedding)\n",
    "        successful_count += 1\n",
    "    \n",
    "    # Check if we have enough embeddings\n",
    "    if successful_count < min_images:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'Insufficient successful embeddings: {successful_count}/{min_images} required',\n",
    "            'successful': successful_count,\n",
    "            'failed': failed_count\n",
    "        }\n",
    "    \n",
    "    # Update global variables (already declared at function start)\n",
    "    \n",
    "    # Add new embeddings to database\n",
    "    if len(new_embeddings) == 0:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'No embeddings extracted from provided images',\n",
    "            'successful': 0,\n",
    "            'failed': failed_count\n",
    "        }\n",
    "    \n",
    "    # Ensure embeddings array has correct shape\n",
    "    new_embeddings_array = np.array(new_embeddings)\n",
    "    if new_embeddings_array.ndim == 1:\n",
    "        # Single embedding case\n",
    "        new_embeddings_array = new_embeddings_array.reshape(1, -1)\n",
    "    \n",
    "    # Verify embedding dimension is 512\n",
    "    if new_embeddings_array.shape[1] != 512:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'Invalid embedding dimension: {new_embeddings_array.shape[1]}, expected 512',\n",
    "            'successful': successful_count,\n",
    "            'failed': failed_count\n",
    "        }\n",
    "    \n",
    "    X = np.vstack([X, new_embeddings_array])\n",
    "    \n",
    "    # Add labels\n",
    "    new_labels = [person_name] * successful_count\n",
    "    y = np.hstack([y, np.array(new_labels)])\n",
    "    \n",
    "    # Update label encoder\n",
    "    label_encoder.fit(y)\n",
    "    y_encoded = label_encoder.transform(y)\n",
    "    \n",
    "    # Update person mappings\n",
    "    if person_name not in person_to_index:\n",
    "        idx = len(person_to_index)\n",
    "        person_to_index[person_name] = idx\n",
    "        index_to_person[idx] = person_name\n",
    "    \n",
    "    # Retrain classifier\n",
    "    print(f\"   üîÑ Retraining classifier with {X.shape[0]} total samples...\")\n",
    "    best_classifier.fit(X, y_encoded)\n",
    "    \n",
    "    # Save updated database\n",
    "    database_path = ARTIFACTS_DIR / \"embedding_database.npy\"\n",
    "    labels_path = ARTIFACTS_DIR / \"labels.npy\"\n",
    "    label_encoder_path = ARTIFACTS_DIR / \"label_encoder.pkl\"\n",
    "    person_mapping_path = ARTIFACTS_DIR / \"person_mapping.json\"\n",
    "    classifier_path = ARTIFACTS_DIR / \"face_classifier.pkl\"\n",
    "    \n",
    "    np.save(database_path, X)\n",
    "    np.save(labels_path, y_encoded)\n",
    "    \n",
    "    with open(label_encoder_path, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    with open(person_mapping_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'person_to_index': person_to_index,\n",
    "            'index_to_person': index_to_person\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    with open(classifier_path, 'wb') as f:\n",
    "        pickle.dump(best_classifier, f)\n",
    "    \n",
    "    print(f\"‚úÖ Person {person_name} registered successfully!\")\n",
    "    print(f\"   Successful embeddings: {successful_count}\")\n",
    "    print(f\"   Failed images: {failed_count}\")\n",
    "    \n",
    "    return {\n",
    "        'success': True,\n",
    "        'person_name': person_name,\n",
    "        'successful': successful_count,\n",
    "        'failed': failed_count,\n",
    "        'total_in_database': X.shape[0]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Register new person function defined\")\n",
    "\n",
    "# Test: Verify function is callable (with empty list, should return error)\n",
    "test_result = register_new_person(\"_test_person\", [], min_images=0)\n",
    "assert isinstance(test_result, dict), \"Registration function failed\"\n",
    "assert 'success' in test_result, \"Test result should contain 'success' key\"\n",
    "# Expected: success=False due to no images provided\n",
    "print(\"‚úÖ Registration function test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816902b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21be4128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running demo on sample dataset image...\n",
      "\n",
      "============================================================\n",
      "FACE RECOGNITION DEMO\n",
      "============================================================\n",
      "\n",
      "üì∏ Processing image: /mnt/NewDisk/sahil_project/FRS copy/Small_Dataset/6204010540/6204010540_A08A0HG3HA_aug19.jpg\n",
      "\n",
      "------------------------------------------------------------\n",
      "RECOGNITION RESULTS\n",
      "------------------------------------------------------------\n",
      "üë§ Identity: 6204010540\n",
      "üéØ Confidence: 1.0000 (100.00%)\n",
      "üìè Angular Distance: 0.0000 radians\n",
      "   (Threshold: 1.0480)\n",
      "üîç Detection Confidence: 0.8407\n",
      "üì¶ Bounding Box: [441, 225, 803, 742]\n",
      "‚ùì Is Unknown: False\n",
      "\n",
      "‚úÖ Person recognized as: 6204010540\n",
      "============================================================\n",
      "\n",
      "‚úÖ All cells completed successfully!\n",
      "\n",
      "üìÅ Artifacts saved to: /mnt/NewDisk/sahil_project/FRS copy/Artifacts\n",
      "   - Embedding database\n",
      "   - Trained classifier\n",
      "   - Recognition thresholds\n",
      "   - Label mappings\n",
      "   - Processing logs\n"
     ]
    }
   ],
   "source": [
    "def demo_recognize(image_path):\n",
    "    \"\"\"\n",
    "    Demo function: Recognize face from image path and print results\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FACE RECOGNITION DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüì∏ Processing image: {image_path}\")\n",
    "    \n",
    "    # Perform recognition\n",
    "    result = recognize_face(str(image_path), return_details=False)\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"RECOGNITION RESULTS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"üë§ Identity: {result['identity']}\")\n",
    "    print(f\"üéØ Confidence: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)\")\n",
    "    \n",
    "    # Handle angular_distance - may be None for errors\n",
    "    if result.get('angular_distance') is not None:\n",
    "        print(f\"üìè Angular Distance: {result['angular_distance']:.4f} radians\")\n",
    "        print(f\"   (Threshold: {RECOGNITION_THRESHOLD:.4f})\")\n",
    "    else:\n",
    "        print(f\"üìè Angular Distance: N/A (error occurred)\")\n",
    "    \n",
    "    print(f\"üîç Detection Confidence: {result.get('detection_confidence', 0.0):.4f}\")\n",
    "    print(f\"üì¶ Bounding Box: {result.get('bbox', None)}\")\n",
    "    print(f\"‚ùì Is Unknown: {result.get('is_unknown', True)}\")\n",
    "    \n",
    "    # Show error if present\n",
    "    if 'error' in result:\n",
    "        print(f\"\\n‚ö†Ô∏è Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    if result['is_unknown']:\n",
    "        print(\"\\n‚ö†Ô∏è  Person not recognized - classified as UNKNOWN\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Person recognized as: {result['identity']}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment and provide image path):\n",
    "# demo_recognize(\"/path/to/test/image.jpg\")\n",
    "\n",
    "# Or test with a dataset image:\n",
    "test_image_for_demo = None\n",
    "for person_dir in DATASET_DIR.iterdir():\n",
    "    if person_dir.is_dir():\n",
    "        image_files = list(person_dir.glob(\"*.jpg\")) + list(person_dir.glob(\"*.png\"))\n",
    "        if image_files:\n",
    "            test_image_for_demo = image_files[0]\n",
    "            break\n",
    "\n",
    "if test_image_for_demo:\n",
    "    print(\"üß™ Running demo on sample dataset image...\\n\")\n",
    "    demo_recognize(str(test_image_for_demo))\n",
    "else:\n",
    "    print(\"‚úÖ Demo function ready\")\n",
    "    print(\"   Usage: demo_recognize('/path/to/image.jpg')\")\n",
    "\n",
    "print(\"\\n‚úÖ All cells completed successfully!\")\n",
    "print(f\"\\nüìÅ Artifacts saved to: {ARTIFACTS_DIR}\")\n",
    "print(f\"   - Embedding database\")\n",
    "print(f\"   - Trained classifier\")\n",
    "print(f\"   - Recognition thresholds\")\n",
    "print(f\"   - Label mappings\")\n",
    "print(f\"   - Processing logs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
